{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.image import ImageDataGenerator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 5\n",
    "img_rows,img_cols = 48,48\n",
    "batch_size = 8\n",
    "\n",
    "train_data_dir = r\"C:\\Users\\Synergiz\\PycharmProjects\\OPEN_Emotion_Rec\\train\"\n",
    "validation_data_dir = r\"C:\\Users\\Synergiz\\PycharmProjects\\OPEN_Emotion_Rec\\validation\"\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "\t\t\t\t\trescale=1./255,\n",
    "\t\t\t\t\trotation_range=30,\n",
    "\t\t\t\t\tshear_range=0.3,\n",
    "\t\t\t\t\tzoom_range=0.3,\n",
    "\t\t\t\t\twidth_shift_range=0.4,\n",
    "\t\t\t\t\theight_shift_range=0.4,\n",
    "\t\t\t\t\thorizontal_flip=True,\n",
    "\t\t\t\t\tfill_mode='nearest')\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "\t\t\t\t\ttrain_data_dir,\n",
    "\t\t\t\t\tcolor_mode='grayscale',\n",
    "\t\t\t\t\ttarget_size=(img_rows,img_cols),\n",
    "\t\t\t\t\tbatch_size=batch_size,\n",
    "\t\t\t\t\tclass_mode='categorical',\n",
    "\t\t\t\t\tshuffle=True)\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "\t\t\t\t\t\t\tvalidation_data_dir,\n",
    "\t\t\t\t\t\t\tcolor_mode='grayscale',\n",
    "\t\t\t\t\t\t\ttarget_size=(img_rows,img_cols),\n",
    "\t\t\t\t\t\t\tbatch_size=batch_size,\n",
    "\t\t\t\t\t\t\tclass_mode='categorical',\n",
    "\t\t\t\t\t\t\tshuffle=True)\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block-1\n",
    "\n",
    "model.add(Conv2D(32,(3,3),padding='same',kernel_initializer='he_normal',input_shape=(img_rows,img_cols,1)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32,(3,3),padding='same',kernel_initializer='he_normal',input_shape=(img_rows,img_cols,1)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Block-2\n",
    "\n",
    "model.add(Conv2D(64,(3,3),padding='same',kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64,(3,3),padding='same',kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Block-3\n",
    "\n",
    "model.add(Conv2D(128,(3,3),padding='same',kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(128,(3,3),padding='same',kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Block-4\n",
    "\n",
    "model.add(Conv2D(256,(3,3),padding='same',kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(256,(3,3),padding='same',kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Block-5\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64,kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Block-6\n",
    "\n",
    "model.add(Dense(64,kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Block-7\n",
    "\n",
    "model.add(Dense(num_classes,kernel_initializer='he_normal'))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(r'C:\\Users\\Synergiz\\PycharmProjects\\OPEN_Emotion_Rec\\Emo_little-h5',\n",
    "                             monitor='val_loss',\n",
    "                             mode='min',\n",
    "                             save_best_only=True,\n",
    "                             verbose=1)\n",
    "\n",
    "earlystop = EarlyStopping(monitor='val_loss',\n",
    "                          min_delta=0,\n",
    "                          patience=3,\n",
    "                          verbose=1,\n",
    "                          restore_best_weights=True\n",
    "                          )\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
    "                              factor=0.2,\n",
    "                              patience=3,\n",
    "                              verbose=1,\n",
    "                              min_delta=0.0001)\n",
    "\n",
    "callbacks = [earlystop,checkpoint,reduce_lr]\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer = Adam(lr=0.001),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "nb_train_samples = 24176\n",
    "nb_validation_samples = 3006\n",
    "epochs=25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history=model.fit_generator(\n",
    "                train_generator,\n",
    "                steps_per_epoch=nb_train_samples//batch_size,\n",
    "                epochs=epochs,\n",
    "                callbacks=callbacks,\n",
    "                validation_data=validation_generator,\n",
    "                validation_steps=nb_validation_samples//batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji={ \"happy\": 0,\"angry\": 1,\"sad\": 2, \"neutral\": 3,\"surprised\": 4,\"lol\": 5 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-bb30cd543ad6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mimg_to_array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "from time import sleep\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.preprocessing import image\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_classifier = cv2.CascadeClassifier(\n",
    "    r'C:\\Users\\Pallavi Shete\\Documents\\Emotion_Detector-main\\Emotion_Detector-main\\haarcascade.xml')\n",
    "classifier = load_model(\n",
    "    r'C:\\Users\\Pallavi Shete\\Documents\\Emotion_Detector-main\\Emotion_Detector-main\\Emo_little-h5.h5')\n",
    "\n",
    "class_labels = ['Angry', 'Happy', 'Neutral', 'Sad', 'Surprise']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    # Grab a single frame of video\n",
    "    ret, frame = cap.read()\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_classifier.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "        roi_gray = gray[y:y + h, x:x + w]\n",
    "        roi_gray = cv2.resize(roi_gray, (48, 48), interpolation=cv2.INTER_AREA)\n",
    "        # rect,face,image = face_detector(frame)\n",
    "\n",
    "        if np.sum([roi_gray]) != 0:\n",
    "            roi = roi_gray.astype('float') / 255.0\n",
    "            roi = img_to_array(roi)\n",
    "            roi = np.expand_dims(roi, axis=0)\n",
    "\n",
    "            # make a prediction on the ROI, then lookup the class\n",
    "\n",
    "            preds = classifier.predict(roi)[0]\n",
    "            label = class_labels[preds.argmax()]\n",
    "            label_position = (x, y)\n",
    "            cv2.putText(frame, label, label_position, cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 255, 0), 3)\n",
    "        else:\n",
    "            cv2.putText(frame, 'No Face Found', (20, 60), cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 255, 0), 3)\n",
    "    cv2.imshow('Emotion Detector', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Emojification #####\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the model in real-time \n",
    "\n",
    "# Importing the libraries\n",
    "import numpy as np\n",
    "from training import get_model, load_trained_model, compile_model\n",
    "import cv2\n",
    "\n",
    "# Load the trained model\n",
    "model = get_model()\n",
    "compile_model(model)\n",
    "load_trained_model(model)\n",
    "\n",
    "# Get frontal face haar cascade\n",
    "#face_classifie = cv2.CascadeClassifier('C:\\Users\\Synergiz\\PycharmProjects\\OPEN_Emotion_Rec\\haarcascade.xml')\n",
    "\n",
    "# Get webcam\n",
    "camera = cv2.VideoCapture(0)\n",
    "\n",
    "# Run the program infinitely\n",
    "while True:\n",
    "    grab_trueorfalse, img = camera.read()       # Read data from the webcam\n",
    "    \n",
    "    # Preprocess input fram webcam\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)        # Convert RGB data to Grayscale\n",
    "    faces = face_classifie.detectMultiScale(gray, 1.3, 5)     # Identify faces in the webcam\n",
    "    \n",
    "    # For each detected face using tha Haar cascade\n",
    "    for (x,y,w,h) in faces:\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "        img_copy = np.copy(img)\n",
    "        img_copy_1 = np.copy(img)\n",
    "        roi_color = img_copy_1[y:y+h, x:x+w]\n",
    "        \n",
    "        width_original = roi_gray.shape[1]      # Width of region where face is detected\n",
    "        height_original = roi_gray.shape[0]     # Height of region where face is detected\n",
    "        img_gray = cv2.resize(roi_gray, (48, 48))       # Resize image to size 96x96\n",
    "        img_gray = img_gray/255         # Normalize the image data\n",
    "        \n",
    "        img_model = np.reshape(img_gray, (1,48,48,1))   # Model takes input of shape = [batch_size, height, width, no. of channels]\n",
    "        keypoints = model.predict(img_model)[0]         # Predict keypoints for the current input\n",
    "        \n",
    "        # Keypoints are saved as (x1, y1, x2, y2, ......)\n",
    "        x_coords = keypoints[0::2]      # Read alternate elements starting from index 0\n",
    "        y_coords = keypoints[1::2]      # Read alternate elements starting from index 1\n",
    "        \n",
    "        x_coords_denormalized = (x_coords+0.5)*width_original       # Denormalize x-coordinate\n",
    "        y_coords_denormalized = (y_coords+0.5)*height_original      # Denormalize y-coordinate\n",
    "        \n",
    "        for i in range(len(x_coords)):          # Plot the keypoints at the x and y coordinates\n",
    "            cv2.circle(roi_color, (x_coords_denormalized[i], y_coords_denormalized[i]), 2, (255,255,0), -1)\n",
    "        \n",
    "      \n",
    "    # Particular keypoints for scaling and positioning of the filter\n",
    "        top = (int(x_coords_denormalized[11]), int(y_coords_denormalized[33]))\n",
    "        bottom = (int(x_coords_denormalized[3]), int(y_coords_denormalized[5]))\n",
    "       \n",
    "        \n",
    "        \n",
    "        # Scale filter according to keypoint coordinates\n",
    "        smiley = top[0] - bottom[0]\n",
    "       \n",
    "        \n",
    "        img_copy = cv2.cvtColor(img_copy, cv2.COLOR_BGR2BGRA)       # Used for transparency overlay of filter using the alpha channel\n",
    "\n",
    "        # Beard filter\n",
    "        face = cv2.imread('emoji/happy.png', -1)\n",
    "        face = cv2.resize(face, (smiley*3,150))\n",
    "        sw,sh,sc = santa_filter.shape\n",
    "        \n",
    "        for i in range(0,sw):       # Overlay the filter based on the alpha channel\n",
    "            for j in range(0,sh):\n",
    "                if np.sum([smiley]) != 0:\n",
    "                    smiley = smiley.astype('float') / 255.0\n",
    "                    smiley = img_to_array(smiley)\n",
    "                    smiley = np.expand_dims(smiley, axis=0)\n",
    "                if smiley[i,j][3] != 0:\n",
    "                    img_copy[top[1]+i+y-20, bottom[0]+j+x-60] = face[i,j]\n",
    "                    \n",
    "      \n",
    "        \n",
    "        img_copy = cv2.cvtColor(img_copy, cv2.COLOR_BGRA2BGR)       # Revert back to BGR\n",
    "        \n",
    "        cv2.imshow('Output',img_copy)           # Output with the filter placed on the face\n",
    "        cv2.imshow('Keypoints predicted',img_copy)        # Place keypoints on the webcam input\n",
    "        \n",
    "    cv2.imshow('Webcam',img)        # Original webcame Input\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):   # If 'q' is pressed, stop reading and break the loop\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
